(sump-venv) (sump-venv) [rda2tc@affogato15 12encoderDreamBigger]$ python trainModel.py 
(23201,)
(20880,)
2022-04-16 15:08:01.855761: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-16 15:08:04.225752: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory:  -> device: 0, name: GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1
2022-04-16 15:08:04.227432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10417 MB memory:  -> device: 1, name: GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1
2022-04-16 15:08:04.228461: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10417 MB memory:  -> device: 2, name: GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1
2022-04-16 15:08:04.229981: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10417 MB memory:  -> device: 3, name: GeForce GTX 1080 Ti, pci bus id: 0000:8a:00.0, compute capability: 6.1
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 64, 1024, 32)      320       
                                                                 
 batch_normalization (BatchN  (None, 64, 1024, 32)     128       
 ormalization)                                                   
                                                                 
 leaky_re_lu (LeakyReLU)     (None, 64, 1024, 32)      0         
                                                                 
 max_pooling2d (MaxPooling2D  (None, 32, 512, 32)      0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 32, 512, 64)       18496     
                                                                 
 batch_normalization_1 (Batc  (None, 32, 512, 64)      256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_1 (LeakyReLU)   (None, 32, 512, 64)       0         
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 16, 256, 64)      0         
 2D)                                                             
                                                                 
 conv2d_2 (Conv2D)           (None, 16, 256, 64)       36928     
                                                                 
 batch_normalization_2 (Batc  (None, 16, 256, 64)      256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_2 (LeakyReLU)   (None, 16, 256, 64)       0         
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 8, 128, 64)       0         
 2D)                                                             
                                                                 
 conv2d_3 (Conv2D)           (None, 8, 128, 64)        36928     
                                                                 
 batch_normalization_3 (Batc  (None, 8, 128, 64)       256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_3 (LeakyReLU)   (None, 8, 128, 64)        0         
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 4, 64, 64)        0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 16384)             0         
                                                                 
 dense (Dense)               (None, 16384)             268451840 
                                                                 
 dense_1 (Dense)             (None, 16384)             268451840 
                                                                 
 reshape (Reshape)           (None, 4, 64, 64)         0         
                                                                 
 conv2d_4 (Conv2D)           (None, 4, 64, 64)         36928     
                                                                 
 batch_normalization_4 (Batc  (None, 4, 64, 64)        256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_4 (LeakyReLU)   (None, 4, 64, 64)         0         
                                                                 
 up_sampling2d (UpSampling2D  (None, 8, 128, 64)       0         
 )                                                               
                                                                 
 conv2d_5 (Conv2D)           (None, 8, 128, 64)        36928     
                                                                 
 batch_normalization_5 (Batc  (None, 8, 128, 64)       256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_5 (LeakyReLU)   (None, 8, 128, 64)        0         
                                                                 
 up_sampling2d_1 (UpSampling  (None, 16, 256, 64)      0         
 2D)                                                             
                                                                 
 conv2d_6 (Conv2D)           (None, 16, 256, 64)       36928     
                                                                 
 batch_normalization_6 (Batc  (None, 16, 256, 64)      256       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_6 (LeakyReLU)   (None, 16, 256, 64)       0         
                                                                 
 up_sampling2d_2 (UpSampling  (None, 32, 512, 64)      0         
 2D)                                                             
                                                                 
 conv2d_7 (Conv2D)           (None, 32, 512, 32)       18464     
                                                                 
 batch_normalization_7 (Batc  (None, 32, 512, 32)      128       
 hNormalization)                                                 
                                                                 
 leaky_re_lu_7 (LeakyReLU)   (None, 32, 512, 32)       0         
                                                                 
 up_sampling2d_3 (UpSampling  (None, 64, 1024, 32)     0         
 2D)                                                             
                                                                 
 conv2d_8 (Conv2D)           (None, 64, 1024, 1)       289       
                                                                 
=================================================================
Total params: 537,127,681
Trainable params: 537,126,785
Non-trainable params: 896
_________________________________________________________________
None
Epoch 1/100
2022-04-16 15:08:06.445932: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101
2022-04-16 15:08:07.250824: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-04-16 15:08:07.343455: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.27GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-04-16 15:08:07.630154: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 3.04GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
652/652 [==============================] - 131s 196ms/step - loss: 0.5858 - val_loss: 0.5825
Epoch 2/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5821 - val_loss: 0.5822
Epoch 3/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5820 - val_loss: 0.5822
Epoch 4/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5820 - val_loss: 0.5822
Epoch 5/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5819 - val_loss: 0.5821
Epoch 6/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5819 - val_loss: 0.5822
Epoch 7/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5819 - val_loss: 0.5821
Epoch 8/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5819 - val_loss: 0.5822
Epoch 9/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 10/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 11/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 12/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 13/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 14/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 15/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 16/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 17/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 18/100
652/652 [==============================] - 131s 201ms/step - loss: 0.5819 - val_loss: 0.5819
Epoch 19/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 20/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 21/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 22/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 23/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5819 - val_loss: 0.5820
Epoch 24/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 25/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 26/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5817 - val_loss: 0.5817
Epoch 27/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5816 - val_loss: 0.5817
Epoch 28/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5816 - val_loss: 0.5820
Epoch 29/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5823
Epoch 30/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5815 - val_loss: 0.5819
Epoch 31/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5818
Epoch 32/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5817
Epoch 33/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5815 - val_loss: 0.5816
Epoch 34/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5815 - val_loss: 0.5817
Epoch 35/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5815 - val_loss: 0.5816
Epoch 36/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5831
Epoch 37/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5816
Epoch 38/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5815 - val_loss: 0.5816
Epoch 39/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5815 - val_loss: 0.5821
Epoch 40/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5818
Epoch 41/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5819
Epoch 42/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5815 - val_loss: 0.5819
Epoch 43/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5836
Epoch 44/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5815 - val_loss: 0.5830
Epoch 45/100
652/652 [==============================] - 135s 207ms/step - loss: 0.5815 - val_loss: 0.5827
Epoch 46/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5815 - val_loss: 0.5816
Epoch 47/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5815 - val_loss: 0.5817
Epoch 48/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5817
Epoch 49/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5819
Epoch 50/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5815 - val_loss: 0.5815
Epoch 51/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5815 - val_loss: 0.5823
Epoch 52/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5814 - val_loss: 0.5824
Epoch 53/100
652/652 [==============================] - 132s 202ms/step - loss: 0.5815 - val_loss: 0.5817
Epoch 54/100
652/652 [==============================] - 188s 289ms/step - loss: 0.5815 - val_loss: 0.5816
Epoch 55/100
652/652 [==============================] - 205s 314ms/step - loss: 0.5815 - val_loss: 0.5825
Epoch 56/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5815 - val_loss: 0.5818
Epoch 57/100
652/652 [==============================] - 133s 204ms/step - loss: 0.5815 - val_loss: 0.5824
Epoch 58/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 59/100
652/652 [==============================] - 132s 202ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 60/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 61/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 62/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 63/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 64/100
652/652 [==============================] - 153s 235ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 65/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 66/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5820
Epoch 67/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 68/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 69/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 70/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 71/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 72/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 73/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 74/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 75/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 76/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 77/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 78/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 79/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 80/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 81/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 82/100
652/652 [==============================] - 130s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 83/100
652/652 [==============================] - 131s 200ms/step - loss: 0.5818 - val_loss: 0.5819
Epoch 84/100
652/652 [==============================] - 130s 199ms/step - loss: 0.5809 - val_loss: 0.5878
Epoch 85/100
652/652 [==============================] - 129s 197ms/step - loss: 0.5744 - val_loss: 0.5729
Epoch 86/100
652/652 [==============================] - 128s 197ms/step - loss: 0.5717 - val_loss: 0.5713
Epoch 87/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5706 - val_loss: 0.5704
Epoch 88/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5698 - val_loss: 0.5698
Epoch 89/100
652/652 [==============================] - 128s 197ms/step - loss: 0.5693 - val_loss: 0.5695
Epoch 90/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5688 - val_loss: 0.5691
Epoch 91/100
652/652 [==============================] - 128s 197ms/step - loss: 0.5684 - val_loss: 0.5689
Epoch 92/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5681 - val_loss: 0.5686
Epoch 93/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5679 - val_loss: 0.5686
Epoch 94/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5676 - val_loss: 0.5684
Epoch 95/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5674 - val_loss: 0.5682
Epoch 96/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5672 - val_loss: 0.5680
Epoch 97/100
652/652 [==============================] - 128s 197ms/step - loss: 0.5671 - val_loss: 0.5679
Epoch 98/100
652/652 [==============================] - 128s 196ms/step - loss: 0.5669 - val_loss: 0.5678
Epoch 99/100
652/652 [==============================] - 129s 198ms/step - loss: 0.5668 - val_loss: 0.5677
Epoch 100/100
652/652 [==============================] - 136s 208ms/step - loss: 0.5666 - val_loss: 0.5677
2022-04-16 18:47:44.972599: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
