(sump-venv) [rda2tc@ai02 15vaeAttempt]$ python trainModel2.py 
(23201,)
2022-04-20 21:39:40.233103: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-20 21:39:42.481302: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 10417 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:04:00.0, compute capability: 6.1
2022-04-20 21:39:42.483077: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 10417 MB memory:  -> device: 1, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:0a:00.0, compute capability: 6.1
2022-04-20 21:39:42.484256: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 10417 MB memory:  -> device: 2, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:84:00.0, compute capability: 6.1
2022-04-20 21:39:42.485858: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 10417 MB memory:  -> device: 3, name: NVIDIA GeForce GTX 1080 Ti, pci bus id: 0000:8a:00.0, compute capability: 6.1
Model: "encoder"
__________________________________________________________________________________________________
 Layer (type)                   Output Shape         Param #     Connected to                     
==================================================================================================
 input_1 (InputLayer)           [(None, 64, 1024, 1  0           []                               
                                )]                                                                
                                                                                                  
 conv2d (Conv2D)                (None, 64, 1024, 32  320         ['input_1[0][0]']                
                                )                                                                 
                                                                                                  
 max_pooling2d (MaxPooling2D)   (None, 32, 512, 32)  0           ['conv2d[0][0]']                 
                                                                                                  
 conv2d_1 (Conv2D)              (None, 32, 512, 64)  18496       ['max_pooling2d[0][0]']          
                                                                                                  
 max_pooling2d_1 (MaxPooling2D)  (None, 16, 256, 64)  0          ['conv2d_1[0][0]']               
                                                                                                  
 conv2d_2 (Conv2D)              (None, 16, 256, 64)  36928       ['max_pooling2d_1[0][0]']        
                                                                                                  
 max_pooling2d_2 (MaxPooling2D)  (None, 8, 128, 64)  0           ['conv2d_2[0][0]']               
                                                                                                  
 conv2d_3 (Conv2D)              (None, 8, 128, 64)   36928       ['max_pooling2d_2[0][0]']        
                                                                                                  
 max_pooling2d_3 (MaxPooling2D)  (None, 4, 64, 64)   0           ['conv2d_3[0][0]']               
                                                                                                  
 flatten (Flatten)              (None, 16384)        0           ['max_pooling2d_3[0][0]']        
                                                                                                  
 dense (Dense)                  (None, 16384)        268451840   ['flatten[0][0]']                
                                                                                                  
 z_mean (Dense)                 (None, 2)            32770       ['dense[0][0]']                  
                                                                                                  
 z_log_var (Dense)              (None, 2)            32770       ['dense[0][0]']                  
                                                                                                  
 sampling (Sampling)            (None, 2)            0           ['z_mean[0][0]',                 
                                                                  'z_log_var[0][0]']              
                                                                                                  
==================================================================================================
Total params: 268,610,052
Trainable params: 268,610,052
Non-trainable params: 0
__________________________________________________________________________________________________
None
Model: "decoder"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 input_2 (InputLayer)        [(None, 2)]               0         
                                                                 
 dense_1 (Dense)             (None, 16384)             49152     
                                                                 
 reshape (Reshape)           (None, 4, 64, 64)         0         
                                                                 
 conv2d_4 (Conv2D)           (None, 4, 64, 64)         36928     
                                                                 
 up_sampling2d (UpSampling2D  (None, 8, 128, 64)       0         
 )                                                               
                                                                 
 conv2d_5 (Conv2D)           (None, 8, 128, 64)        36928     
                                                                 
 up_sampling2d_1 (UpSampling  (None, 16, 256, 64)      0         
 2D)                                                             
                                                                 
 conv2d_6 (Conv2D)           (None, 16, 256, 64)       36928     
                                                                 
 up_sampling2d_2 (UpSampling  (None, 32, 512, 64)      0         
 2D)                                                             
                                                                 
 conv2d_7 (Conv2D)           (None, 32, 512, 32)       18464     
                                                                 
 up_sampling2d_3 (UpSampling  (None, 64, 1024, 32)     0         
 2D)                                                             
                                                                 
 conv2d_transpose_1 (Conv2DT  (None, 64, 1024, 1)      289       
 ranspose)                                                       
                                                                 
=================================================================
Total params: 178,689
Trainable params: 178,689
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/100
2022-04-20 21:39:47.731162: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101
2022-04-20 21:39:49.629495: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2022-04-20 21:39:49.673186: W tensorflow/core/common_runtime/bfc_allocator.cc:275] Allocator (GPU_0_bfc) ran out of memory trying to allocate 2.34GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
181/181 [==============================] - 86s 435ms/step - loss: 53691.0584 - reconstruction_loss: 41569.3906 - kl_loss: 1374.2186
Epoch 2/100
181/181 [==============================] - 81s 436ms/step - loss: 38350.9616 - reconstruction_loss: 38291.6992 - kl_loss: 15.8110
Epoch 3/100
181/181 [==============================] - 83s 437ms/step - loss: 38168.2103 - reconstruction_loss: 38111.3633 - kl_loss: 10.2933
Epoch 4/100
181/181 [==============================] - 81s 435ms/step - loss: 38057.0038 - reconstruction_loss: 38035.6211 - kl_loss: 8.9971
Epoch 5/100
181/181 [==============================] - 82s 439ms/step - loss: 38015.6984 - reconstruction_loss: 38007.0938 - kl_loss: 8.1449
Epoch 6/100
181/181 [==============================] - 84s 445ms/step - loss: 38001.7294 - reconstruction_loss: 37990.6484 - kl_loss: 7.7291
Epoch 7/100
181/181 [==============================] - 81s 430ms/step - loss: 37981.5809 - reconstruction_loss: 37980.9531 - kl_loss: 7.4616
Epoch 8/100
181/181 [==============================] - 83s 439ms/step - loss: 37956.4854 - reconstruction_loss: 37958.0820 - kl_loss: 7.3570
Epoch 9/100
181/181 [==============================] - 81s 438ms/step - loss: 37962.1338 - reconstruction_loss: 37953.8555 - kl_loss: 7.2551
Epoch 10/100
181/181 [==============================] - 83s 435ms/step - loss: 37953.6145 - reconstruction_loss: 37946.7812 - kl_loss: 7.1391
Epoch 11/100
181/181 [==============================] - 81s 430ms/step - loss: 37950.6322 - reconstruction_loss: 37940.1172 - kl_loss: 7.0130
Epoch 12/100
181/181 [==============================] - 84s 440ms/step - loss: 37937.6270 - reconstruction_loss: 37937.0742 - kl_loss: 6.9536
Epoch 13/100
181/181 [==============================] - 81s 436ms/step - loss: 37939.4520 - reconstruction_loss: 37932.4805 - kl_loss: 6.8681
Epoch 14/100
181/181 [==============================] - 82s 440ms/step - loss: 37931.2973 - reconstruction_loss: 37926.3047 - kl_loss: 6.8557
Epoch 15/100
181/181 [==============================] - 81s 439ms/step - loss: 37932.2690 - reconstruction_loss: 37919.8594 - kl_loss: 6.7945
Epoch 16/100
181/181 [==============================] - 83s 436ms/step - loss: 37934.1051 - reconstruction_loss: 37918.8281 - kl_loss: 6.7518
Epoch 17/100
181/181 [==============================] - 81s 437ms/step - loss: 37924.2326 - reconstruction_loss: 37916.1250 - kl_loss: 6.7177
Epoch 18/100
181/181 [==============================] - 83s 443ms/step - loss: 37911.6173 - reconstruction_loss: 37910.9844 - kl_loss: 6.7390
Epoch 19/100
181/181 [==============================] - 81s 438ms/step - loss: 37916.7558 - reconstruction_loss: 37908.8242 - kl_loss: 6.6502
Epoch 20/100
181/181 [==============================] - 83s 435ms/step - loss: 37913.1649 - reconstruction_loss: 37905.5117 - kl_loss: 6.6598
Epoch 21/100
181/181 [==============================] - 82s 439ms/step - loss: 37910.1588 - reconstruction_loss: 37902.4180 - kl_loss: 6.6319
Epoch 22/100
181/181 [==============================] - 81s 442ms/step - loss: 37915.7166 - reconstruction_loss: 37904.6055 - kl_loss: 6.6195
Epoch 23/100
181/181 [==============================] - 84s 437ms/step - loss: 37900.1992 - reconstruction_loss: 37895.7422 - kl_loss: 6.6392
Epoch 24/100
181/181 [==============================] - 82s 443ms/step - loss: 37905.0120 - reconstruction_loss: 37892.9492 - kl_loss: 6.6436
Epoch 25/100
181/181 [==============================] - 81s 436ms/step - loss: 37896.3741 - reconstruction_loss: 37893.6172 - kl_loss: 6.6334
Epoch 26/100
181/181 [==============================] - 83s 435ms/step - loss: 37890.4774 - reconstruction_loss: 37887.7305 - kl_loss: 6.6664
Epoch 27/100
181/181 [==============================] - 81s 439ms/step - loss: 37890.3740 - reconstruction_loss: 37886.0430 - kl_loss: 6.6980
Epoch 28/100
181/181 [==============================] - 83s 435ms/step - loss: 37888.8420 - reconstruction_loss: 37881.8086 - kl_loss: 6.7160
Epoch 29/100
181/181 [==============================] - 81s 435ms/step - loss: 37896.9591 - reconstruction_loss: 37880.9805 - kl_loss: 6.7469
Epoch 30/100
181/181 [==============================] - 83s 440ms/step - loss: 37885.5878 - reconstruction_loss: 37877.8750 - kl_loss: 6.7475
Epoch 31/100
181/181 [==============================] - 81s 437ms/step - loss: 37877.8383 - reconstruction_loss: 37873.1211 - kl_loss: 6.8295
Epoch 32/100
181/181 [==============================] - 84s 448ms/step - loss: 37868.1032 - reconstruction_loss: 37874.1289 - kl_loss: 6.8433
Epoch 33/100
181/181 [==============================] - 82s 437ms/step - loss: 37878.1904 - reconstruction_loss: 37869.1367 - kl_loss: 6.8530
Epoch 34/100
181/181 [==============================] - 84s 447ms/step - loss: 37873.1824 - reconstruction_loss: 37866.2344 - kl_loss: 6.8796
Epoch 35/100
181/181 [==============================] - 81s 435ms/step - loss: 37877.3399 - reconstruction_loss: 37867.6445 - kl_loss: 6.9126
Epoch 36/100
181/181 [==============================] - 85s 441ms/step - loss: 37866.3019 - reconstruction_loss: 37863.7617 - kl_loss: 6.9439
Epoch 37/100
181/181 [==============================] - 82s 442ms/step - loss: 37865.9135 - reconstruction_loss: 37860.4961 - kl_loss: 6.9850
Epoch 38/100
181/181 [==============================] - 81s 441ms/step - loss: 37871.9277 - reconstruction_loss: 37859.4570 - kl_loss: 6.9788
Epoch 39/100
181/181 [==============================] - 84s 441ms/step - loss: 37858.2251 - reconstruction_loss: 37857.7500 - kl_loss: 7.0323
Epoch 40/100
181/181 [==============================] - 82s 437ms/step - loss: 37858.8816 - reconstruction_loss: 37853.9883 - kl_loss: 7.0443
Epoch 41/100
181/181 [==============================] - 83s 440ms/step - loss: 37854.5644 - reconstruction_loss: 37853.0039 - kl_loss: 7.1043
Epoch 42/100
181/181 [==============================] - 80s 435ms/step - loss: 37861.7144 - reconstruction_loss: 37850.5039 - kl_loss: 7.1149
Epoch 43/100
181/181 [==============================] - 82s 444ms/step - loss: 37851.2909 - reconstruction_loss: 37851.1562 - kl_loss: 7.1606
Epoch 44/100
181/181 [==============================] - 84s 439ms/step - loss: 37855.1409 - reconstruction_loss: 37847.0703 - kl_loss: 7.1774
Epoch 45/100
181/181 [==============================] - 81s 441ms/step - loss: 37841.7570 - reconstruction_loss: 37845.7227 - kl_loss: 7.2158
Epoch 46/100
181/181 [==============================] - 84s 443ms/step - loss: 37852.9607 - reconstruction_loss: 37842.8828 - kl_loss: 7.2171
Epoch 47/100
181/181 [==============================] - 81s 439ms/step - loss: 37843.9614 - reconstruction_loss: 37843.7422 - kl_loss: 7.2347
Epoch 48/100
181/181 [==============================] - 84s 435ms/step - loss: 37843.4481 - reconstruction_loss: 37843.2344 - kl_loss: 7.2595
Epoch 49/100
181/181 [==============================] - 81s 434ms/step - loss: 37845.5680 - reconstruction_loss: 37837.6836 - kl_loss: 7.2895
Epoch 50/100
181/181 [==============================] - 83s 437ms/step - loss: 37848.1243 - reconstruction_loss: 37837.0469 - kl_loss: 7.3179
Epoch 51/100
181/181 [==============================] - 82s 438ms/step - loss: 37839.4099 - reconstruction_loss: 37836.8555 - kl_loss: 7.3492
Epoch 52/100
181/181 [==============================] - 84s 438ms/step - loss: 37837.1014 - reconstruction_loss: 37833.2969 - kl_loss: 7.3851
Epoch 53/100
181/181 [==============================] - 82s 441ms/step - loss: 37843.3789 - reconstruction_loss: 37833.0859 - kl_loss: 7.4253
Epoch 54/100
181/181 [==============================] - 84s 441ms/step - loss: 37839.1949 - reconstruction_loss: 37831.8633 - kl_loss: 7.4569
Epoch 55/100
181/181 [==============================] - 82s 441ms/step - loss: 37839.1538 - reconstruction_loss: 37829.5703 - kl_loss: 7.4430
Epoch 56/100
181/181 [==============================] - 80s 432ms/step - loss: 37839.4942 - reconstruction_loss: 37828.7305 - kl_loss: 7.4200
Epoch 57/100
181/181 [==============================] - 83s 439ms/step - loss: 37832.1884 - reconstruction_loss: 37826.0312 - kl_loss: 7.5086
Epoch 58/100
181/181 [==============================] - 82s 440ms/step - loss: 37830.5014 - reconstruction_loss: 37825.7031 - kl_loss: 7.5033
Epoch 59/100
181/181 [==============================] - 82s 442ms/step - loss: 37834.9522 - reconstruction_loss: 37823.7695 - kl_loss: 7.5503
Epoch 60/100
181/181 [==============================] - 84s 444ms/step - loss: 37834.3577 - reconstruction_loss: 37823.5742 - kl_loss: 7.5742
Epoch 61/100
181/181 [==============================] - 80s 432ms/step - loss: 37824.3011 - reconstruction_loss: 37820.6836 - kl_loss: 7.5631
Epoch 62/100
181/181 [==============================] - 83s 441ms/step - loss: 37820.1236 - reconstruction_loss: 37818.3242 - kl_loss: 7.6248
Epoch 63/100
181/181 [==============================] - 82s 440ms/step - loss: 37815.6133 - reconstruction_loss: 37819.3984 - kl_loss: 7.6199
Epoch 64/100
181/181 [==============================] - 82s 443ms/step - loss: 37825.7191 - reconstruction_loss: 37817.0977 - kl_loss: 7.6698
Epoch 65/100
181/181 [==============================] - 84s 441ms/step - loss: 37835.6025 - reconstruction_loss: 37816.1680 - kl_loss: 7.6642
Epoch 66/100
181/181 [==============================] - 84s 444ms/step - loss: 37843.0793 - reconstruction_loss: 37816.2305 - kl_loss: 7.6816
Epoch 67/100
181/181 [==============================] - 81s 441ms/step - loss: 37816.8663 - reconstruction_loss: 37814.6953 - kl_loss: 7.7006
Epoch 68/100
181/181 [==============================] - 84s 441ms/step - loss: 37818.9860 - reconstruction_loss: 37812.9883 - kl_loss: 7.7213
Epoch 69/100
181/181 [==============================] - 81s 434ms/step - loss: 37824.4507 - reconstruction_loss: 37812.5898 - kl_loss: 7.7275
Epoch 70/100
181/181 [==============================] - 83s 443ms/step - loss: 37814.6169 - reconstruction_loss: 37809.9492 - kl_loss: 7.7645
Epoch 71/100
181/181 [==============================] - 81s 439ms/step - loss: 37818.3023 - reconstruction_loss: 37808.6328 - kl_loss: 7.7609
Epoch 72/100
181/181 [==============================] - 84s 440ms/step - loss: 37820.9752 - reconstruction_loss: 37807.4531 - kl_loss: 7.8175
Epoch 73/100
181/181 [==============================] - 84s 444ms/step - loss: 37815.9826 - reconstruction_loss: 37807.8750 - kl_loss: 7.8242
Epoch 74/100
181/181 [==============================] - 84s 438ms/step - loss: 37816.6108 - reconstruction_loss: 37806.2578 - kl_loss: 7.8298
Epoch 75/100
181/181 [==============================] - 82s 438ms/step - loss: 37815.3625 - reconstruction_loss: 37805.8555 - kl_loss: 7.8364
Epoch 76/100
181/181 [==============================] - 81s 439ms/step - loss: 37813.6893 - reconstruction_loss: 37804.4688 - kl_loss: 7.8653
Epoch 77/100
181/181 [==============================] - 83s 440ms/step - loss: 37807.1780 - reconstruction_loss: 37802.0312 - kl_loss: 7.9016
Epoch 78/100
181/181 [==============================] - 81s 436ms/step - loss: 37815.0019 - reconstruction_loss: 37802.6133 - kl_loss: 7.9038
Epoch 79/100
181/181 [==============================] - 84s 441ms/step - loss: 37817.6058 - reconstruction_loss: 37800.5039 - kl_loss: 7.9256
Epoch 80/100
181/181 [==============================] - 80s 432ms/step - loss: 37816.7693 - reconstruction_loss: 37801.7070 - kl_loss: 7.9337
Epoch 81/100
181/181 [==============================] - 83s 436ms/step - loss: 37818.7020 - reconstruction_loss: 37799.3438 - kl_loss: 7.9507
Epoch 82/100
181/181 [==============================] - 79s 423ms/step - loss: 37801.6080 - reconstruction_loss: 37800.6914 - kl_loss: 7.9993
Epoch 83/100
181/181 [==============================] - 82s 437ms/step - loss: 37805.0029 - reconstruction_loss: 37798.6797 - kl_loss: 7.9799
Epoch 84/100
181/181 [==============================] - 83s 436ms/step - loss: 37802.2258 - reconstruction_loss: 37796.7422 - kl_loss: 7.9830
Epoch 85/100
181/181 [==============================] - 84s 443ms/step - loss: 37802.7578 - reconstruction_loss: 37796.2344 - kl_loss: 8.0319
Epoch 86/100
181/181 [==============================] - 82s 442ms/step - loss: 37811.1584 - reconstruction_loss: 37794.8438 - kl_loss: 8.0697
Epoch 87/100
181/181 [==============================] - 84s 442ms/step - loss: 37815.4602 - reconstruction_loss: 37794.3906 - kl_loss: 8.0552
Epoch 88/100
181/181 [==============================] - 81s 438ms/step - loss: 37796.3096 - reconstruction_loss: 37792.4805 - kl_loss: 8.0872
Epoch 89/100
181/181 [==============================] - 81s 439ms/step - loss: 37803.4932 - reconstruction_loss: 37792.5234 - kl_loss: 8.0994
Epoch 90/100
181/181 [==============================] - 83s 442ms/step - loss: 37795.7146 - reconstruction_loss: 37791.6289 - kl_loss: 8.0864
Epoch 91/100
181/181 [==============================] - 81s 432ms/step - loss: 37804.1268 - reconstruction_loss: 37791.3516 - kl_loss: 8.1522
Epoch 92/100
181/181 [==============================] - 83s 442ms/step - loss: 37796.6671 - reconstruction_loss: 37788.9180 - kl_loss: 8.1104
Epoch 93/100
181/181 [==============================] - 81s 436ms/step - loss: 37797.1026 - reconstruction_loss: 37789.4023 - kl_loss: 8.1346
Epoch 94/100
181/181 [==============================] - 83s 442ms/step - loss: 37797.0804 - reconstruction_loss: 37789.1055 - kl_loss: 8.1542
Epoch 95/100
181/181 [==============================] - 81s 435ms/step - loss: 37797.6723 - reconstruction_loss: 37788.4023 - kl_loss: 8.1636
Epoch 96/100
181/181 [==============================] - 83s 438ms/step - loss: 37805.8755 - reconstruction_loss: 37786.6562 - kl_loss: 8.1949
Epoch 97/100
181/181 [==============================] - 81s 436ms/step - loss: 37801.2123 - reconstruction_loss: 37788.0039 - kl_loss: 8.2012
Epoch 98/100
181/181 [==============================] - 83s 441ms/step - loss: 37793.7455 - reconstruction_loss: 37786.1133 - kl_loss: 8.2058
Epoch 99/100
181/181 [==============================] - 82s 436ms/step - loss: 37794.6464 - reconstruction_loss: 37785.8594 - kl_loss: 8.2255
Epoch 100/100
181/181 [==============================] - 84s 443ms/step - loss: 37791.9551 - reconstruction_loss: 37783.6641 - kl_loss: 8.2244
WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.
2022-04-21 00:42:29.844673: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in th