(sp_venv) [rda2tc@cheetah01 12enc8resize]$ cd v27/
(sp_venv) [rda2tc@cheetah01 v27]$ python trainModel.py
(40000,)
(28000,)
2022-04-30 10:14:17.780662: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-30 10:14:22.031786: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0
2022-04-30 10:14:22.081491: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 38420 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0
2022-04-30 10:14:22.083373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 38420 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0
2022-04-30 10:14:22.085280: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 38420 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 64, 1024, 32)      320       
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 1024, 32)      9248      
                                                                 
 max_pooling2d (MaxPooling2D  (None, 32, 512, 32)      0         
 )                                                               
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 512, 64)       18496     
                                                                 
 conv2d_3 (Conv2D)           (None, 32, 512, 64)       36928     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 16, 256, 64)      0         
 2D)                                                             
                                                                 
 conv2d_4 (Conv2D)           (None, 16, 256, 256)      147712    
                                                                 
 conv2d_5 (Conv2D)           (None, 16, 256, 256)      590080    
                                                                 
 conv2d_6 (Conv2D)           (None, 16, 256, 256)      590080    
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 8, 128, 256)      0         
 2D)                                                             
                                                                 
 conv2d_7 (Conv2D)           (None, 8, 128, 256)       590080    
                                                                 
 conv2d_8 (Conv2D)           (None, 8, 128, 256)       590080    
                                                                 
 conv2d_9 (Conv2D)           (None, 8, 128, 256)       590080    
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 4, 64, 256)       0         
 2D)                                                             
                                                                 
 conv2d_10 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 conv2d_11 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 conv2d_12 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 2, 32, 256)       0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 16384)             0         
                                                                 
 dense (Dense)               (None, 16384)             268451840 
                                                                 
 dense_1 (Dense)             (None, 16384)             268451840 
                                                                 
 reshape (Reshape)           (None, 2, 32, 256)        0         
                                                                 
 conv2d_13 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 conv2d_14 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 conv2d_15 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 up_sampling2d (UpSampling2D  (None, 4, 64, 256)       0         
 )                                                               
                                                                 
 conv2d_16 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 conv2d_17 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 conv2d_18 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 up_sampling2d_1 (UpSampling  (None, 8, 128, 256)      0         
 2D)                                                             
                                                                 
 conv2d_19 (Conv2D)          (None, 8, 128, 256)       590080    
                                                                 
 conv2d_20 (Conv2D)          (None, 8, 128, 256)       590080    
                                                                 
 conv2d_21 (Conv2D)          (None, 8, 128, 256)       590080    
                                                                 
 up_sampling2d_2 (UpSampling  (None, 16, 256, 256)     0         
 2D)                                                             
                                                                 
 conv2d_22 (Conv2D)          (None, 16, 256, 64)       147520    
                                                                 
 conv2d_23 (Conv2D)          (None, 16, 256, 64)       36928     
                                                                 
 up_sampling2d_3 (UpSampling  (None, 32, 512, 64)      0         
 2D)                                                             
                                                                 
 conv2d_24 (Conv2D)          (None, 32, 512, 32)       18464     
                                                                 
 conv2d_25 (Conv2D)          (None, 32, 512, 32)       9248      
                                                                 
 up_sampling2d_4 (UpSampling  (None, 64, 1024, 32)     0         
 2D)                                                             
                                                                 
 conv2d_26 (Conv2D)          (None, 64, 1024, 1)       289       
                                                                 
=================================================================
Total params: 547,360,193
Trainable params: 547,360,193
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/200
2022-04-30 10:14:29.066213: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101
2022-04-30 10:14:31.472521: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
218/218 [==============================] - 592s 3s/step - loss: 0.6231 - val_loss: 0.5868
Epoch 2/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5831 - val_loss: 0.5821
Epoch 3/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5821 - val_loss: 0.5816
Epoch 4/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5817 - val_loss: 0.5816
Epoch 5/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5815 - val_loss: 0.5814
Epoch 6/200
218/218 [==============================] - 86s 392ms/step - loss: 0.5814 - val_loss: 0.5814
Epoch 7/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5814 - val_loss: 0.5812
Epoch 8/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5813 - val_loss: 0.5813
Epoch 9/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5813 - val_loss: 0.5812
Epoch 10/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 11/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5813 - val_loss: 0.5811
Epoch 12/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5812 - val_loss: 0.5812
Epoch 13/200
218/218 [==============================] - 83s 381ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 14/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5812 - val_loss: 0.5813
Epoch 15/200
218/218 [==============================] - 83s 378ms/step - loss: 0.5812 - val_loss: 0.5810
Epoch 16/200
218/218 [==============================] - 86s 393ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 17/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5812 - val_loss: 0.5810
Epoch 18/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 19/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 20/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 21/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 22/200
218/218 [==============================] - 83s 380ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 23/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 24/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 25/200
218/218 [==============================] - 138s 634ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 26/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 27/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 28/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 29/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 30/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 31/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 32/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 33/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 34/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 35/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 36/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 37/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 38/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 39/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 40/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 41/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5827 - val_loss: 0.6037
Epoch 42/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5897 - val_loss: 0.5849
Epoch 43/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5848 - val_loss: 0.5847
Epoch 44/200
218/218 [==============================] - 82s 373ms/step - loss: 0.5847 - val_loss: 0.5847
Epoch 45/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 46/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5847 - val_loss: 0.5847
Epoch 47/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 48/200
218/218 [==============================] - 81s 372ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 49/200
218/218 [==============================] - 83s 380ms/step - loss: 0.5847 - val_loss: 0.5847
Epoch 50/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 51/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 52/200
218/218 [==============================] - 82s 377ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 53/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 54/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5847 - val_loss: 0.5846
Epoch 55/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 56/200
218/218 [==============================] - 85s 389ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 57/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 58/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 59/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 60/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 61/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 62/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 63/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 64/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 65/200
218/218 [==============================] - 83s 382ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 66/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 67/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 68/200
218/218 [==============================] - 81s 371ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 69/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5848
Epoch 70/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 71/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 72/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 73/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 74/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 75/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 76/200
218/218 [==============================] - 81s 372ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 77/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 78/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 79/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 80/200
218/218 [==============================] - 81s 370ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 81/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 82/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 83/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 84/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 85/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 86/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 87/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 88/200
218/218 [==============================] - 83s 382ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 89/200
218/218 [==============================] - 89s 409ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 90/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 91/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 92/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 93/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 94/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 95/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 96/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 97/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 98/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 99/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 100/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 101/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 102/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 103/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 104/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5848 - val_loss: 0.5846
Epoch 105/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 106/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 107/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 108/200
218/218 [==============================] - 82s 373ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 109/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 110/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 111/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 112/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 113/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 114/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 115/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 116/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 117/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 118/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 119/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 120/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 121/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 122/200
218/218 [==============================] - 82s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 123/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 124/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 125/200
218/218 [==============================] - 82s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 126/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 127/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 128/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 129/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 130/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 131/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 132/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 133/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 134/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 135/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 136/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 137/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 138/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 139/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 140/200
218/218 [==============================] - 82s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 141/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 142/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 143/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 144/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 145/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 146/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 147/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 148/200
218/218 [==============================] - 87s 400ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 149/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 150/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 151/200
218/218 [==============================] - 85s 391ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 152/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 153/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 154/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 155/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 156/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 157/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 158/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 159/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 160/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 161/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 162/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 163/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 164/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 165/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 166/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 167/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 168/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 169/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 170/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 171/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 172/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 173/200
218/218 [==============================] - 81s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 174/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 175/200
218/218 [==============================] - 83s 381ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 176/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 177/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 178/200
218/218 [==============================] - 83s 380ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 179/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 180/200
218/218 [==============================] - 82s 373ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 181/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5846
Epoch 182/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 183/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 184/200
218/218 [==============================] - 83s 380ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 185/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 186/200
218/218 [==============================] - 82s 374ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 187/200
218/218 [==============================] - 100s 457ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 188/200
218/218 [==============================] - 99s 453ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 189/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 190/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 191/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 192/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 193/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 194/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 195/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 196/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 197/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 198/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 199/200
218/218 [==============================] - 82s 375ms/step - loss: 0.5846 - val_loss: 0.5845
Epoch 200/200
218/218 [==============================] - 82s 376ms/step - loss: 0.5846 - val_loss: 0.5845
2022-04-30 14:57:32.513930: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
(sp_venv) [rda2tc@cheetah01 v27]$ 
