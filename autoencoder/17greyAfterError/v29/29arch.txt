sp_venv) [rda2tc@cheetah01 v29]$ python trainModel.py 
(40000,)
(32000,)
2022-04-30 17:00:00.847151: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-30 17:00:02.838922: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 38420 MB memory:  -> device: 0, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:01:00.0, compute capability: 8.0
2022-04-30 17:00:02.840879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 33471 MB memory:  -> device: 1, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:41:00.0, compute capability: 8.0
2022-04-30 17:00:02.842503: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 33067 MB memory:  -> device: 2, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:81:00.0, compute capability: 8.0
2022-04-30 17:00:02.844123: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 33289 MB memory:  -> device: 3, name: NVIDIA A100-PCIE-40GB, pci bus id: 0000:c1:00.0, compute capability: 8.0
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 16, 256, 96)       11712     
                                                                 
 batch_normalization (BatchN  (None, 16, 256, 96)      384       
 ormalization)                                                   
                                                                 
 max_pooling2d (MaxPooling2D  (None, 8, 128, 96)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 8, 128, 256)       614656    
                                                                 
 batch_normalization_1 (Batc  (None, 8, 128, 256)      1024      
 hNormalization)                                                 
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 4, 64, 256)       0         
 2D)                                                             
                                                                 
 conv2d_2 (Conv2D)           (None, 4, 64, 384)        885120    
                                                                 
 batch_normalization_2 (Batc  (None, 4, 64, 384)       1536      
 hNormalization)                                                 
                                                                 
 conv2d_3 (Conv2D)           (None, 4, 64, 384)        1327488   
                                                                 
 batch_normalization_3 (Batc  (None, 4, 64, 384)       1536      
 hNormalization)                                                 
                                                                 
 conv2d_4 (Conv2D)           (None, 4, 64, 256)        884992    
                                                                 
 batch_normalization_4 (Batc  (None, 4, 64, 256)       1024      
 hNormalization)                                                 
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 2, 32, 256)       0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 16384)             0         
                                                                 
 dense (Dense)               (None, 16384)             268451840 
                                                                 
 dense_1 (Dense)             (None, 16384)             268451840 
                                                                 
 reshape (Reshape)           (None, 2, 32, 256)        0         
                                                                 
 conv2d_5 (Conv2D)           (None, 2, 32, 256)        590080    
                                                                 
 batch_normalization_5 (Batc  (None, 2, 32, 256)       1024      
 hNormalization)                                                 
                                                                 
 up_sampling2d (UpSampling2D  (None, 4, 64, 256)       0         
 )                                                               
                                                                 
 conv2d_6 (Conv2D)           (None, 4, 64, 384)        885120    
                                                                 
 batch_normalization_6 (Batc  (None, 4, 64, 384)       1536      
 hNormalization)                                                 
                                                                 
 up_sampling2d_1 (UpSampling  (None, 8, 128, 384)      0         
 2D)                                                             
                                                                 
 conv2d_7 (Conv2D)           (None, 8, 128, 384)       1327488   
                                                                 
 batch_normalization_7 (Batc  (None, 8, 128, 384)      1536      
 hNormalization)                                                 
                                                                 
 up_sampling2d_2 (UpSampling  (None, 16, 256, 384)     0         
 2D)                                                             
                                                                 
 conv2d_8 (Conv2D)           (None, 16, 256, 256)      2457856   
                                                                 
 batch_normalization_8 (Batc  (None, 16, 256, 256)     1024      
 hNormalization)                                                 
                                                                 
 up_sampling2d_3 (UpSampling  (None, 32, 512, 256)     0         
 2D)                                                             
                                                                 
 conv2d_9 (Conv2D)           (None, 32, 512, 96)       2973792   
                                                                 
 batch_normalization_9 (Batc  (None, 32, 512, 96)      384       
 hNormalization)                                                 
                                                                 
 up_sampling2d_4 (UpSampling  (None, 64, 1024, 96)     0         
 2D)                                                             
                                                                 
 conv2d_10 (Conv2D)          (None, 64, 1024, 1)       865       
                                                                 
=================================================================
Total params: 548,873,857
Trainable params: 548,868,353
Non-trainable params: 5,504
_________________________________________________________________
None
Epoch 1/200
2022-04-30 17:00:10.005085: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101
2022-04-30 17:00:12.329072: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.
250/250 [==============================] - 258s 881ms/step - loss: 0.6353 - val_loss: 0.5885
Epoch 2/200
250/250 [==============================] - 197s 780ms/step - loss: 0.5831 - val_loss: 0.5833
Epoch 3/200
250/250 [==============================] - 196s 780ms/step - loss: 0.5820 - val_loss: 0.5820
Epoch 4/200
250/250 [==============================] - 196s 781ms/step - loss: 0.5816 - val_loss: 0.5813
Epoch 5/200
250/250 [==============================] - 196s 781ms/step - loss: 0.5814 - val_loss: 0.5812
Epoch 6/200
250/250 [==============================] - 196s 781ms/step - loss: 0.5813 - val_loss: 0.5818
Epoch 7/200
250/250 [==============================] - 197s 782ms/step - loss: 0.5809 - val_loss: 0.5808
Epoch 8/200
250/250 [==============================] - 197s 783ms/step - loss: 0.5807 - val_loss: 0.5820
Epoch 9/200
250/250 [==============================] - 197s 781ms/step - loss: 0.5805 - val_loss: 0.5808
Epoch 10/200
250/250 [==============================] - 196s 781ms/step - loss: 0.5804 - val_loss: 0.5831
Epoch 11/200
250/250 [==============================] - 197s 783ms/step - loss: 0.5803 - val_loss: 0.5803
Epoch 12/200
250/250 [==============================] - 196s 781ms/step - loss: 0.5803 - val_loss: 0.5804
Epoch 13/200
250/250 [==============================] - 196s 781ms/step - loss: 0.5803 - val_loss: 0.5831
Epoch 14/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5802 - val_loss: 0.5801
Epoch 15/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5802 - val_loss: 0.5803
Epoch 16/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5802 - val_loss: 0.5801
Epoch 17/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5802 - val_loss: 0.5816
Epoch 18/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5798 - val_loss: 0.5801
Epoch 19/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5796 - val_loss: 0.5806
Epoch 20/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5795 - val_loss: 0.5796
Epoch 21/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5795 - val_loss: 0.5802
Epoch 22/200
250/250 [==============================] - 195s 779ms/step - loss: 0.5794 - val_loss: 0.5798
Epoch 23/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5794 - val_loss: 0.5822
Epoch 24/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5794 - val_loss: 0.5806
Epoch 25/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5794 - val_loss: 0.5797
Epoch 26/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5793 - val_loss: 0.5819
Epoch 27/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5793 - val_loss: 0.5796
Epoch 28/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5793 - val_loss: 0.5808
Epoch 29/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5793 - val_loss: 0.5802
Epoch 30/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5793 - val_loss: 0.5799
Epoch 31/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5793 - val_loss: 0.5806
Epoch 32/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5793 - val_loss: 0.5796
Epoch 33/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5793 - val_loss: 0.5842
Epoch 34/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5793 - val_loss: 0.5799
Epoch 35/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5793 - val_loss: 0.5794
Epoch 36/200
250/250 [==============================] - 196s 780ms/step - loss: 0.5793 - val_loss: 0.5810
Epoch 37/200
250/250 [==============================] - 196s 780ms/step - loss: 0.5793 - val_loss: 0.5840
Epoch 38/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5792 - val_loss: 0.5799
Epoch 39/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5793 - val_loss: 0.5807
Epoch 40/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5793 - val_loss: 0.5799
Epoch 41/200
250/250 [==============================] - 195s 779ms/step - loss: 0.5793 - val_loss: 0.5807
Epoch 42/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5793 - val_loss: 0.5842
Epoch 43/200
250/250 [==============================] - 196s 779ms/step - loss: 0.5792 - val_loss: 0.5813
Epoch 44/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5795 - val_loss: 0.5811
Epoch 45/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5797 - val_loss: 0.5833
Epoch 46/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5801
Epoch 47/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5797 - val_loss: 0.5816
Epoch 48/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5797 - val_loss: 0.5805
Epoch 49/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5812
Epoch 50/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5797 - val_loss: 0.5805
Epoch 51/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5797 - val_loss: 0.5854
Epoch 52/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5812
Epoch 53/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5869
Epoch 54/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5804
Epoch 55/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5796 - val_loss: 0.5812
Epoch 56/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5815
Epoch 57/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5799 - val_loss: 0.5803
Epoch 58/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5797 - val_loss: 0.5803
Epoch 59/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5797 - val_loss: 0.5834
Epoch 60/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5824
Epoch 61/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5797 - val_loss: 0.5848
Epoch 62/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5796 - val_loss: 0.5807
Epoch 63/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5799 - val_loss: 0.5804
Epoch 64/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5802 - val_loss: 0.5807
Epoch 65/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5803 - val_loss: 0.5815
Epoch 66/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5803 - val_loss: 0.5807
Epoch 67/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5802 - val_loss: 0.5809
Epoch 68/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5802 - val_loss: 0.5803
Epoch 69/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5803 - val_loss: 0.5818
Epoch 70/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5802 - val_loss: 0.5837
Epoch 71/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5807 - val_loss: 0.5834
Epoch 72/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 73/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 74/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 75/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 76/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 77/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 78/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 79/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 80/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 81/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 82/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 83/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 84/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 85/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 86/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 87/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 88/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 89/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 90/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 91/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 92/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 93/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 94/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 95/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 96/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 97/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 98/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 99/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 100/200
250/250 [==============================] - 196s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 101/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 102/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 103/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 104/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 105/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 106/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 107/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 108/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 109/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 110/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 111/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 112/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 113/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 114/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 115/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 116/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 117/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 118/200
250/250 [==============================] - 195s 773ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 119/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 120/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 121/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 122/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 123/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 124/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 125/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 126/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 127/200
250/250 [==============================] - 196s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 128/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 129/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 130/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 131/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 132/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 133/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 134/200
250/250 [==============================] - 195s 773ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 135/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 136/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 137/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 138/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 139/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 140/200
250/250 [==============================] - 194s 773ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 141/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 142/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 143/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 144/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 145/200
250/250 [==============================] - 194s 773ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 146/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 147/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 148/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 149/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 150/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 151/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 152/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 153/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 154/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 155/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 156/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 157/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 158/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 159/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 160/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 161/200
250/250 [==============================] - 194s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 162/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 163/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 164/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 165/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 166/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 167/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 168/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 169/200
250/250 [==============================] - 194s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 170/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 171/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 172/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 173/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 174/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 175/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 176/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 177/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 178/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 179/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 180/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 181/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 182/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 183/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 184/200
250/250 [==============================] - 195s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 185/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 186/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 187/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 188/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 189/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 190/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 191/200
250/250 [==============================] - 195s 774ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 192/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 193/200
250/250 [==============================] - 195s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 194/200
250/250 [==============================] - 196s 778ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 195/200
250/250 [==============================] - 196s 777ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 196/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 197/200
250/250 [==============================] - 195s 778ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 198/200
250/250 [==============================] - 196s 775ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 199/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
Epoch 200/200
250/250 [==============================] - 195s 776ms/step - loss: 0.5811 - val_loss: 0.5809
2022-05-01 03:54:05.974849: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
(sp_venv) [rda2tc@cheetah01