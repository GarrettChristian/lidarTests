[rda2tc@adriatic04 v5]$ spm11
(sp_venv) [rda2tc@adriatic04 v5]$ python trainModel.py 
(40000,)
(28000,)
2022-04-29 18:09:31.311311: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2022-04-29 18:09:36.773640: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 6674 MB memory:  -> device: 0, name: Quadro RTX 4000, pci bus id: 0000:3d:00.0, compute capability: 7.5
2022-04-29 18:09:36.842897: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 6674 MB memory:  -> device: 1, name: Quadro RTX 4000, pci bus id: 0000:60:00.0, compute capability: 7.5
2022-04-29 18:09:36.846038: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:2 with 6674 MB memory:  -> device: 2, name: Quadro RTX 4000, pci bus id: 0000:b1:00.0, compute capability: 7.5
2022-04-29 18:09:36.849450: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1525] Created device /job:localhost/replica:0/task:0/device:GPU:3 with 6674 MB memory:  -> device: 3, name: Quadro RTX 4000, pci bus id: 0000:da:00.0, compute capability: 7.5
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 64, 1024, 32)      320       
                                                                 
 conv2d_1 (Conv2D)           (None, 64, 1024, 32)      9248      
                                                                 
 max_pooling2d (MaxPooling2D  (None, 32, 512, 32)      0         
 )                                                               
                                                                 
 conv2d_2 (Conv2D)           (None, 32, 512, 64)       18496     
                                                                 
 conv2d_3 (Conv2D)           (None, 32, 512, 64)       36928     
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 16, 256, 64)      0         
 2D)                                                             
                                                                 
 conv2d_4 (Conv2D)           (None, 16, 256, 256)      147712    
                                                                 
 conv2d_5 (Conv2D)           (None, 16, 256, 256)      590080    
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 8, 128, 256)      0         
 2D)                                                             
                                                                 
 conv2d_6 (Conv2D)           (None, 8, 128, 256)       590080    
                                                                 
 conv2d_7 (Conv2D)           (None, 8, 128, 256)       590080    
                                                                 
 max_pooling2d_3 (MaxPooling  (None, 4, 64, 256)       0         
 2D)                                                             
                                                                 
 conv2d_8 (Conv2D)           (None, 4, 64, 256)        590080    
                                                                 
 conv2d_9 (Conv2D)           (None, 4, 64, 256)        590080    
                                                                 
 max_pooling2d_4 (MaxPooling  (None, 2, 32, 256)       0         
 2D)                                                             
                                                                 
 conv2d_10 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 conv2d_11 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 max_pooling2d_5 (MaxPooling  (None, 1, 16, 256)       0         
 2D)                                                             
                                                                 
 flatten (Flatten)           (None, 4096)              0         
                                                                 
 dense (Dense)               (None, 4096)              16781312  
                                                                 
 dense_1 (Dense)             (None, 4096)              16781312  
                                                                 
 reshape (Reshape)           (None, 1, 16, 256)        0         
                                                                 
 conv2d_12 (Conv2D)          (None, 1, 16, 256)        590080    
                                                                 
 conv2d_13 (Conv2D)          (None, 1, 16, 256)        590080    
                                                                 
 up_sampling2d (UpSampling2D  (None, 2, 32, 256)       0         
 )                                                               
                                                                 
 conv2d_14 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 conv2d_15 (Conv2D)          (None, 2, 32, 256)        590080    
                                                                 
 up_sampling2d_1 (UpSampling  (None, 4, 64, 256)       0         
 2D)                                                             
                                                                 
 conv2d_16 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 conv2d_17 (Conv2D)          (None, 4, 64, 256)        590080    
                                                                 
 up_sampling2d_2 (UpSampling  (None, 8, 128, 256)      0         
 2D)                                                             
                                                                 
 conv2d_18 (Conv2D)          (None, 8, 128, 256)       590080    
                                                                 
 conv2d_19 (Conv2D)          (None, 8, 128, 256)       590080    
                                                                 
 up_sampling2d_3 (UpSampling  (None, 16, 256, 256)     0         
 2D)                                                             
                                                                 
 conv2d_20 (Conv2D)          (None, 16, 256, 64)       147520    
                                                                 
 conv2d_21 (Conv2D)          (None, 16, 256, 64)       36928     
                                                                 
 up_sampling2d_4 (UpSampling  (None, 32, 512, 64)      0         
 2D)                                                             
                                                                 
 conv2d_22 (Conv2D)          (None, 32, 512, 32)       18464     
                                                                 
 conv2d_23 (Conv2D)          (None, 32, 512, 32)       9248      
                                                                 
 up_sampling2d_5 (UpSampling  (None, 64, 1024, 32)     0         
 2D)                                                             
                                                                 
 conv2d_24 (Conv2D)          (None, 64, 1024, 1)       289       
                                                                 
=================================================================
Total params: 42,838,977
Trainable params: 42,838,977
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/200
2022-04-29 18:09:43.992147: I tensorflow/stream_executor/cuda/cuda_dnn.cc:368] Loaded cuDNN version 8101
875/875 [==============================] - 723s 803ms/step - loss: 0.5924 - val_loss: 0.5815
Epoch 2/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5816 - val_loss: 0.5814
Epoch 3/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5814 - val_loss: 0.5811
Epoch 4/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5813 - val_loss: 0.5811
Epoch 5/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 6/200
875/875 [==============================] - 250s 286ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 7/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5812 - val_loss: 0.5812
Epoch 8/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 9/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5812 - val_loss: 0.5811
Epoch 10/200
875/875 [==============================] - 250s 286ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 11/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 12/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 13/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 14/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 15/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 16/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 17/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 18/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 19/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 20/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 21/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 22/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 23/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 24/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 25/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 26/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 27/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 28/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 29/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 30/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 31/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 32/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 33/200
875/875 [==============================] - 249s 284ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 34/200
875/875 [==============================] - 250s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 35/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 36/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 37/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 38/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 39/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 40/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 41/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 42/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 43/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 44/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 45/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 46/200
875/875 [==============================] - 249s 284ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 47/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 48/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 49/200
875/875 [==============================] - 249s 284ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 50/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 51/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 52/200
875/875 [==============================] - 249s 285ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 53/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5811
Epoch 54/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 55/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 56/200
875/875 [==============================] - 253s 290ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 57/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 58/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 59/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 60/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 61/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 62/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 63/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 64/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 65/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 66/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 67/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 68/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 69/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 70/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 71/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 72/200
875/875 [==============================] - 248s 283ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 73/200
875/875 [==============================] - 248s 283ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 74/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 75/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 76/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 77/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 78/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 79/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 80/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 81/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 82/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 83/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 84/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 85/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 86/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 87/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 88/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 89/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 90/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 91/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 92/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 93/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 94/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 95/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 96/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 97/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 98/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 99/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 100/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 101/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 102/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 103/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 104/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 105/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 106/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 107/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 108/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 109/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 110/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 111/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 112/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 113/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 114/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 115/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 116/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 117/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 118/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 119/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 120/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 121/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 122/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 123/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 124/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 125/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 126/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 127/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 128/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 129/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 130/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 131/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 132/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 133/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 134/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 135/200
875/875 [==============================] - 253s 289ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 136/200
875/875 [==============================] - 251s 286ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 137/200
875/875 [==============================] - 250s 286ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 138/200
875/875 [==============================] - 252s 288ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 139/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 140/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 141/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 142/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 143/200
875/875 [==============================] - 252s 288ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 144/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 145/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 146/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 147/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 148/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 149/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 150/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 151/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 152/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 153/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 154/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 155/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 156/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 157/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 158/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 159/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 160/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 161/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 162/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 163/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 164/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 165/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 166/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 167/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 168/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 169/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 170/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 171/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 172/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 173/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 174/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 175/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 176/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 177/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 178/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 179/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 180/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 181/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 182/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 183/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 184/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 185/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 186/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 187/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5811 - val_loss: 0.5810
Epoch 188/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 189/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 190/200
875/875 [==============================] - 247s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 191/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 192/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 193/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 194/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 195/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 196/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 197/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 198/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 199/200
875/875 [==============================] - 246s 282ms/step - loss: 0.5810 - val_loss: 0.5810
Epoch 200/200
875/875 [==============================] - 246s 281ms/step - loss: 0.5810 - val_loss: 0.5810
2022-04-30 08:02:19.712003: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
(sp_venv) [rda2tc@adriatic04 v5]$ 