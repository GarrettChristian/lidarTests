(sum_proj) autoencoder/16smallerModels - [main●] » python trainModel4.py
(40000,)
(32000,)
2022-04-15 17:50:49.516270: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
Model: "sequential"
_________________________________________________________________
 Layer (type)                Output Shape              Param #   
=================================================================
 conv2d (Conv2D)             (None, 64, 64, 16)        160       
                                                                 
 max_pooling2d (MaxPooling2D  (None, 32, 32, 16)       0         
 )                                                               
                                                                 
 conv2d_1 (Conv2D)           (None, 32, 32, 8)         1160      
                                                                 
 max_pooling2d_1 (MaxPooling  (None, 16, 16, 8)        0         
 2D)                                                             
                                                                 
 conv2d_2 (Conv2D)           (None, 16, 16, 8)         584       
                                                                 
 max_pooling2d_2 (MaxPooling  (None, 8, 8, 8)          0         
 2D)                                                             
                                                                 
 conv2d_3 (Conv2D)           (None, 8, 8, 8)           584       
                                                                 
 up_sampling2d (UpSampling2D  (None, 16, 16, 8)        0         
 )                                                               
                                                                 
 conv2d_4 (Conv2D)           (None, 16, 16, 8)         584       
                                                                 
 up_sampling2d_1 (UpSampling  (None, 32, 32, 8)        0         
 2D)                                                             
                                                                 
 conv2d_5 (Conv2D)           (None, 32, 32, 16)        1168      
                                                                 
 up_sampling2d_2 (UpSampling  (None, 64, 64, 16)       0         
 2D)                                                             
                                                                 
 conv2d_6 (Conv2D)           (None, 64, 64, 1)         145       
                                                                 
=================================================================
Total params: 4,385
Trainable params: 4,385
Non-trainable params: 0
_________________________________________________________________
None
Epoch 1/200
1000/1000 [==============================] - 134s 130ms/step - loss: 0.3025 - val_loss: 0.2716
Epoch 2/200
1000/1000 [==============================] - 178s 175ms/step - loss: 0.2657 - val_loss: 0.2606
Epoch 3/200
1000/1000 [==============================] - 188s 184ms/step - loss: 0.2573 - val_loss: 0.2561
Epoch 4/200
1000/1000 [==============================] - 191s 188ms/step - loss: 0.2517 - val_loss: 0.2497
Epoch 5/200
1000/1000 [==============================] - 196s 193ms/step - loss: 0.2479 - val_loss: 0.2459
Epoch 6/200
1000/1000 [==============================] - 178s 174ms/step - loss: 0.2449 - val_loss: 0.2431
Epoch 7/200
1000/1000 [==============================] - 177s 174ms/step - loss: 0.2424 - val_loss: 0.2413
Epoch 8/200
1000/1000 [==============================] - 188s 183ms/step - loss: 0.2405 - val_loss: 0.2394
Epoch 9/200
1000/1000 [==============================] - 186s 182ms/step - loss: 0.2389 - val_loss: 0.2384
Epoch 10/200
1000/1000 [==============================] - 197s 191ms/step - loss: 0.2375 - val_loss: 0.2369
Epoch 11/200
1000/1000 [==============================] - 172s 169ms/step - loss: 0.2363 - val_loss: 0.2361
Epoch 12/200
1000/1000 [==============================] - 178s 174ms/step - loss: 0.2353 - val_loss: 0.2347
Epoch 13/200
1000/1000 [==============================] - 168s 165ms/step - loss: 0.2343 - val_loss: 0.2342
Epoch 14/200
1000/1000 [==============================] - 197s 191ms/step - loss: 0.2332 - val_loss: 0.2330
Epoch 15/200
1000/1000 [==============================] - 201s 196ms/step - loss: 0.2323 - val_loss: 0.2319
Epoch 16/200
1000/1000 [==============================] - 183s 179ms/step - loss: 0.2314 - val_loss: 0.2319
Epoch 17/200
1000/1000 [==============================] - 185s 181ms/step - loss: 0.2305 - val_loss: 0.2317
Epoch 18/200
1000/1000 [==============================] - 190s 186ms/step - loss: 0.2297 - val_loss: 0.2290
Epoch 19/200
1000/1000 [==============================] - 180s 177ms/step - loss: 0.2289 - val_loss: 0.2285
Epoch 20/200
1000/1000 [==============================] - 193s 189ms/step - loss: 0.2282 - val_loss: 0.2277
Epoch 21/200
1000/1000 [==============================] - 176s 173ms/step - loss: 0.2277 - val_loss: 0.2270
Epoch 22/200
1000/1000 [==============================] - 205s 201ms/step - loss: 0.2271 - val_loss: 0.2266
Epoch 23/200
1000/1000 [==============================] - 243s 238ms/step - loss: 0.2266 - val_loss: 0.2263
Epoch 24/200
1000/1000 [==============================] - 247s 242ms/step - loss: 0.2261 - val_loss: 0.2263
Epoch 25/200
1000/1000 [==============================] - 292s 288ms/step - loss: 0.2258 - val_loss: 0.2255
Epoch 26/200
1000/1000 [==============================] - 243s 238ms/step - loss: 0.2254 - val_loss: 0.2255
Epoch 27/200
1000/1000 [==============================] - 252s 246ms/step - loss: 0.2250 - val_loss: 0.2247
Epoch 28/200
1000/1000 [==============================] - 256s 251ms/step - loss: 0.2248 - val_loss: 0.2255
Epoch 29/200
1000/1000 [==============================] - 269s 264ms/step - loss: 0.2245 - val_loss: 0.2240
Epoch 30/200
1000/1000 [==============================] - 287s 281ms/step - loss: 0.2242 - val_loss: 0.2238
Epoch 31/200
1000/1000 [==============================] - 218s 213ms/step - loss: 0.2240 - val_loss: 0.2236
Epoch 32/200
1000/1000 [==============================] - 224s 221ms/step - loss: 0.2237 - val_loss: 0.2234
Epoch 33/200
1000/1000 [==============================] - 223s 216ms/step - loss: 0.2236 - val_loss: 0.2231
Epoch 34/200
1000/1000 [==============================] - 227s 223ms/step - loss: 0.2233 - val_loss: 0.2229
Epoch 35/200
1000/1000 [==============================] - 240s 236ms/step - loss: 0.2231 - val_loss: 0.2242
Epoch 36/200
1000/1000 [==============================] - 212s 208ms/step - loss: 0.2229 - val_loss: 0.2224
Epoch 37/200
1000/1000 [==============================] - 226s 222ms/step - loss: 0.2228 - val_loss: 0.2223
Epoch 38/200
1000/1000 [==============================] - 239s 235ms/step - loss: 0.2226 - val_loss: 0.2225
Epoch 39/200
1000/1000 [==============================] - 252s 247ms/step - loss: 0.2224 - val_loss: 0.2220
Epoch 40/200
1000/1000 [==============================] - 238s 233ms/step - loss: 0.2222 - val_loss: 0.2220
Epoch 41/200
1000/1000 [==============================] - 301s 297ms/step - loss: 0.2221 - val_loss: 0.2217
Epoch 42/200
1000/1000 [==============================] - 242s 236ms/step - loss: 0.2219 - val_loss: 0.2232
Epoch 43/200
1000/1000 [==============================] - 238s 233ms/step - loss: 0.2218 - val_loss: 0.2219
Epoch 44/200
1000/1000 [==============================] - 229s 223ms/step - loss: 0.2216 - val_loss: 0.2211
Epoch 45/200
1000/1000 [==============================] - 230s 225ms/step - loss: 0.2214 - val_loss: 0.2214
Epoch 46/200
1000/1000 [==============================] - 227s 223ms/step - loss: 0.2212 - val_loss: 0.2220
Epoch 47/200
1000/1000 [==============================] - 238s 233ms/step - loss: 0.2211 - val_loss: 0.2207
Epoch 48/200
1000/1000 [==============================] - 274s 270ms/step - loss: 0.2210 - val_loss: 0.2204
Epoch 49/200
1000/1000 [==============================] - 227s 222ms/step - loss: 0.2208 - val_loss: 0.2205
Epoch 50/200
1000/1000 [==============================] - 239s 234ms/step - loss: 0.2206 - val_loss: 0.2237
Epoch 51/200
1000/1000 [==============================] - 251s 246ms/step - loss: 0.2206 - val_loss: 0.2202
Epoch 52/200
1000/1000 [==============================] - 259s 254ms/step - loss: 0.2204 - val_loss: 0.2200
Epoch 53/200
1000/1000 [==============================] - 235s 230ms/step - loss: 0.2203 - val_loss: 0.2198
Epoch 54/200
1000/1000 [==============================] - 241s 236ms/step - loss: 0.2203 - val_loss: 0.2197
Epoch 55/200
1000/1000 [==============================] - 246s 241ms/step - loss: 0.2201 - val_loss: 0.2204
Epoch 56/200
1000/1000 [==============================] - 234s 229ms/step - loss: 0.2200 - val_loss: 0.2210
Epoch 57/200
1000/1000 [==============================] - 234s 230ms/step - loss: 0.2199 - val_loss: 0.2205
Epoch 58/200
1000/1000 [==============================] - 245s 240ms/step - loss: 0.2198 - val_loss: 0.2195
Epoch 59/200
1000/1000 [==============================] - 255s 251ms/step - loss: 0.2197 - val_loss: 0.2194
Epoch 60/200
1000/1000 [==============================] - 234s 229ms/step - loss: 0.2197 - val_loss: 0.2192
Epoch 61/200
1000/1000 [==============================] - 275s 271ms/step - loss: 0.2196 - val_loss: 0.2192
Epoch 62/200
1000/1000 [==============================] - 287s 281ms/step - loss: 0.2195 - val_loss: 0.2190
Epoch 63/200
1000/1000 [==============================] - 333s 320ms/step - loss: 0.2194 - val_loss: 0.2190
Epoch 64/200
1000/1000 [==============================] - 284s 279ms/step - loss: 0.2194 - val_loss: 0.2203
Epoch 65/200
1000/1000 [==============================] - 279s 272ms/step - loss: 0.2192 - val_loss: 0.2190
Epoch 66/200
1000/1000 [==============================] - 155s 152ms/step - loss: 0.2192 - val_loss: 0.2189
Epoch 67/200
1000/1000 [==============================] - 203s 200ms/step - loss: 0.2191 - val_loss: 0.2188
Epoch 68/200
1000/1000 [==============================] - 192s 188ms/step - loss: 0.2191 - val_loss: 0.2188
Epoch 69/200
1000/1000 [==============================] - 189s 185ms/step - loss: 0.2190 - val_loss: 0.2188
Epoch 70/200
1000/1000 [==============================] - 148s 144ms/step - loss: 0.2189 - val_loss: 0.2187
Epoch 71/200
1000/1000 [==============================] - 164s 160ms/step - loss: 0.2189 - val_loss: 0.2191
Epoch 72/200
1000/1000 [==============================] - 209s 204ms/step - loss: 0.2189 - val_loss: 0.2185
Epoch 73/200
1000/1000 [==============================] - 246s 242ms/step - loss: 0.2188 - val_loss: 0.2187
Epoch 74/200
1000/1000 [==============================] - 215s 211ms/step - loss: 0.2187 - val_loss: 0.2184
Epoch 75/200
1000/1000 [==============================] - 246s 240ms/step - loss: 0.2187 - val_loss: 0.2197
Epoch 76/200
1000/1000 [==============================] - 185s 181ms/step - loss: 0.2186 - val_loss: 0.2184
Epoch 77/200
1000/1000 [==============================] - 210s 206ms/step - loss: 0.2186 - val_loss: 0.2196
Epoch 78/200
1000/1000 [==============================] - 282s 277ms/step - loss: 0.2186 - val_loss: 0.2185
Epoch 79/200
1000/1000 [==============================] - 227s 224ms/step - loss: 0.2185 - val_loss: 0.2195
Epoch 80/200
1000/1000 [==============================] - 196s 192ms/step - loss: 0.2184 - val_loss: 0.2179
Epoch 81/200
1000/1000 [==============================] - 181s 177ms/step - loss: 0.2184 - val_loss: 0.2183
Epoch 82/200
1000/1000 [==============================] - 172s 169ms/step - loss: 0.2183 - val_loss: 0.2182
Epoch 83/200
1000/1000 [==============================] - 184s 180ms/step - loss: 0.2183 - val_loss: 0.2178
Epoch 84/200
1000/1000 [==============================] - 148s 145ms/step - loss: 0.2183 - val_loss: 0.2181
Epoch 85/200
1000/1000 [==============================] - 185s 182ms/step - loss: 0.2182 - val_loss: 0.2181
Epoch 86/200
1000/1000 [==============================] - 216s 210ms/step - loss: 0.2182 - val_loss: 0.2177
Epoch 87/200
1000/1000 [==============================] - 164s 160ms/step - loss: 0.2182 - val_loss: 0.2186
Epoch 88/200
1000/1000 [==============================] - 168s 165ms/step - loss: 0.2181 - val_loss: 0.2200
Epoch 89/200
1000/1000 [==============================] - 161s 158ms/step - loss: 0.2181 - val_loss: 0.2196
Epoch 90/200
1000/1000 [==============================] - 154s 150ms/step - loss: 0.2180 - val_loss: 0.2181
Epoch 91/200
1000/1000 [==============================] - 150s 147ms/step - loss: 0.2180 - val_loss: 0.2177
Epoch 92/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2180 - val_loss: 0.2183
Epoch 93/200
1000/1000 [==============================] - 125s 122ms/step - loss: 0.2180 - val_loss: 0.2180
Epoch 94/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2179 - val_loss: 0.2177
Epoch 95/200
1000/1000 [==============================] - 147s 143ms/step - loss: 0.2179 - val_loss: 0.2178
Epoch 96/200
1000/1000 [==============================] - 144s 140ms/step - loss: 0.2179 - val_loss: 0.2180
Epoch 97/200
1000/1000 [==============================] - 149s 146ms/step - loss: 0.2178 - val_loss: 0.2175
Epoch 98/200
1000/1000 [==============================] - 151s 148ms/step - loss: 0.2178 - val_loss: 0.2181
Epoch 99/200
1000/1000 [==============================] - 135s 132ms/step - loss: 0.2178 - val_loss: 0.2178
Epoch 100/200
1000/1000 [==============================] - 123s 120ms/step - loss: 0.2177 - val_loss: 0.2176
Epoch 101/200
1000/1000 [==============================] - 144s 141ms/step - loss: 0.2177 - val_loss: 0.2180
Epoch 102/200
1000/1000 [==============================] - 133s 130ms/step - loss: 0.2176 - val_loss: 0.2172
Epoch 103/200
1000/1000 [==============================] - 147s 144ms/step - loss: 0.2176 - val_loss: 0.2172
Epoch 104/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2175 - val_loss: 0.2170
Epoch 105/200
1000/1000 [==============================] - 141s 137ms/step - loss: 0.2175 - val_loss: 0.2176
Epoch 106/200
1000/1000 [==============================] - 143s 140ms/step - loss: 0.2174 - val_loss: 0.2177
Epoch 107/200
1000/1000 [==============================] - 133s 130ms/step - loss: 0.2174 - val_loss: 0.2174
Epoch 108/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2174 - val_loss: 0.2171
Epoch 109/200
1000/1000 [==============================] - 137s 134ms/step - loss: 0.2174 - val_loss: 0.2170
Epoch 110/200
1000/1000 [==============================] - 127s 124ms/step - loss: 0.2174 - val_loss: 0.2170
Epoch 111/200
1000/1000 [==============================] - 135s 132ms/step - loss: 0.2173 - val_loss: 0.2175
Epoch 112/200
1000/1000 [==============================] - 121s 118ms/step - loss: 0.2173 - val_loss: 0.2171
Epoch 113/200
1000/1000 [==============================] - 140s 137ms/step - loss: 0.2173 - val_loss: 0.2170
Epoch 114/200
1000/1000 [==============================] - 130s 127ms/step - loss: 0.2173 - val_loss: 0.2178
Epoch 115/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2172 - val_loss: 0.2172
Epoch 116/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2172 - val_loss: 0.2181
Epoch 117/200
1000/1000 [==============================] - 136s 132ms/step - loss: 0.2172 - val_loss: 0.2169
Epoch 118/200
1000/1000 [==============================] - 123s 120ms/step - loss: 0.2171 - val_loss: 0.2176
Epoch 119/200
1000/1000 [==============================] - 140s 137ms/step - loss: 0.2171 - val_loss: 0.2176
Epoch 120/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2171 - val_loss: 0.2175
Epoch 121/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2171 - val_loss: 0.2166
Epoch 122/200
1000/1000 [==============================] - 140s 136ms/step - loss: 0.2170 - val_loss: 0.2165
Epoch 123/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2170 - val_loss: 0.2172
Epoch 124/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2170 - val_loss: 0.2168
Epoch 125/200
1000/1000 [==============================] - 132s 129ms/step - loss: 0.2170 - val_loss: 0.2165
Epoch 126/200
1000/1000 [==============================] - 135s 131ms/step - loss: 0.2169 - val_loss: 0.2168
Epoch 127/200
1000/1000 [==============================] - 131s 128ms/step - loss: 0.2169 - val_loss: 0.2167
Epoch 128/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2169 - val_loss: 0.2166
Epoch 129/200
1000/1000 [==============================] - 135s 132ms/step - loss: 0.2169 - val_loss: 0.2164
Epoch 130/200
1000/1000 [==============================] - 134s 131ms/step - loss: 0.2169 - val_loss: 0.2171
Epoch 131/200
1000/1000 [==============================] - 137s 134ms/step - loss: 0.2169 - val_loss: 0.2168
Epoch 132/200
1000/1000 [==============================] - 134s 130ms/step - loss: 0.2168 - val_loss: 0.2164
Epoch 133/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2168 - val_loss: 0.2167
Epoch 134/200
1000/1000 [==============================] - 133s 130ms/step - loss: 0.2168 - val_loss: 0.2163
Epoch 135/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2168 - val_loss: 0.2164
Epoch 136/200
1000/1000 [==============================] - 140s 137ms/step - loss: 0.2168 - val_loss: 0.2164
Epoch 137/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2168 - val_loss: 0.2167
Epoch 138/200
1000/1000 [==============================] - 142s 138ms/step - loss: 0.2167 - val_loss: 0.2171
Epoch 139/200
1000/1000 [==============================] - 139s 135ms/step - loss: 0.2167 - val_loss: 0.2166
Epoch 140/200
1000/1000 [==============================] - 150s 147ms/step - loss: 0.2167 - val_loss: 0.2163
Epoch 141/200
1000/1000 [==============================] - 132s 129ms/step - loss: 0.2167 - val_loss: 0.2172
Epoch 142/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2167 - val_loss: 0.2184
Epoch 143/200
1000/1000 [==============================] - 142s 139ms/step - loss: 0.2167 - val_loss: 0.2164
Epoch 144/200
1000/1000 [==============================] - 133s 130ms/step - loss: 0.2166 - val_loss: 0.2167
Epoch 145/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2166 - val_loss: 0.2167
Epoch 146/200
1000/1000 [==============================] - 140s 137ms/step - loss: 0.2166 - val_loss: 0.2166
Epoch 147/200
1000/1000 [==============================] - 137s 134ms/step - loss: 0.2166 - val_loss: 0.2165
Epoch 148/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2166 - val_loss: 0.2163
Epoch 149/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2166 - val_loss: 0.2166
Epoch 150/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2166 - val_loss: 0.2161
Epoch 151/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2165 - val_loss: 0.2161
Epoch 152/200
1000/1000 [==============================] - 122s 119ms/step - loss: 0.2166 - val_loss: 0.2165
Epoch 153/200
1000/1000 [==============================] - 126s 123ms/step - loss: 0.2165 - val_loss: 0.2164
Epoch 154/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2165 - val_loss: 0.2171
Epoch 155/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2165 - val_loss: 0.2160
Epoch 156/200
1000/1000 [==============================] - 136s 133ms/step - loss: 0.2165 - val_loss: 0.2160
Epoch 157/200
1000/1000 [==============================] - 142s 139ms/step - loss: 0.2165 - val_loss: 0.2162
Epoch 158/200
1000/1000 [==============================] - 138s 134ms/step - loss: 0.2165 - val_loss: 0.2169
Epoch 159/200
1000/1000 [==============================] - 137s 134ms/step - loss: 0.2165 - val_loss: 0.2163
Epoch 160/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2164 - val_loss: 0.2163
Epoch 161/200
1000/1000 [==============================] - 137s 134ms/step - loss: 0.2165 - val_loss: 0.2166
Epoch 162/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2164 - val_loss: 0.2160
Epoch 163/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2164 - val_loss: 0.2161
Epoch 164/200
1000/1000 [==============================] - 144s 141ms/step - loss: 0.2164 - val_loss: 0.2159
Epoch 165/200
1000/1000 [==============================] - 143s 140ms/step - loss: 0.2164 - val_loss: 0.2163
Epoch 166/200
1000/1000 [==============================] - 134s 131ms/step - loss: 0.2163 - val_loss: 0.2181
Epoch 167/200
1000/1000 [==============================] - 132s 129ms/step - loss: 0.2164 - val_loss: 0.2159
Epoch 168/200
1000/1000 [==============================] - 127s 124ms/step - loss: 0.2163 - val_loss: 0.2164
Epoch 169/200
1000/1000 [==============================] - 127s 124ms/step - loss: 0.2164 - val_loss: 0.2163
Epoch 170/200
1000/1000 [==============================] - 129s 126ms/step - loss: 0.2163 - val_loss: 0.2163
Epoch 171/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2163 - val_loss: 0.2170
Epoch 172/200
1000/1000 [==============================] - 142s 139ms/step - loss: 0.2163 - val_loss: 0.2164
Epoch 173/200
1000/1000 [==============================] - 148s 145ms/step - loss: 0.2163 - val_loss: 0.2159
Epoch 174/200
1000/1000 [==============================] - 144s 141ms/step - loss: 0.2163 - val_loss: 0.2160
Epoch 175/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2163 - val_loss: 0.2163
Epoch 176/200
1000/1000 [==============================] - 137s 134ms/step - loss: 0.2163 - val_loss: 0.2163
Epoch 177/200
1000/1000 [==============================] - 145s 142ms/step - loss: 0.2163 - val_loss: 0.2159
Epoch 178/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2162 - val_loss: 0.2159
Epoch 179/200
1000/1000 [==============================] - 144s 141ms/step - loss: 0.2162 - val_loss: 0.2159
Epoch 180/200
1000/1000 [==============================] - 143s 139ms/step - loss: 0.2162 - val_loss: 0.2158
Epoch 181/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2162 - val_loss: 0.2161
Epoch 182/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2162 - val_loss: 0.2161
Epoch 183/200
1000/1000 [==============================] - 135s 131ms/step - loss: 0.2162 - val_loss: 0.2162
Epoch 184/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2162 - val_loss: 0.2167
Epoch 185/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2162 - val_loss: 0.2161
Epoch 186/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2162 - val_loss: 0.2159
Epoch 187/200
1000/1000 [==============================] - 138s 135ms/step - loss: 0.2161 - val_loss: 0.2159
Epoch 188/200
1000/1000 [==============================] - 133s 130ms/step - loss: 0.2162 - val_loss: 0.2161
Epoch 189/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2161 - val_loss: 0.2160
Epoch 190/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2162 - val_loss: 0.2165
Epoch 191/200
1000/1000 [==============================] - 144s 141ms/step - loss: 0.2161 - val_loss: 0.2160
Epoch 192/200
1000/1000 [==============================] - 137s 133ms/step - loss: 0.2162 - val_loss: 0.2157
Epoch 193/200
1000/1000 [==============================] - 141s 138ms/step - loss: 0.2161 - val_loss: 0.2158
Epoch 194/200
1000/1000 [==============================] - 140s 138ms/step - loss: 0.2161 - val_loss: 0.2161
Epoch 195/200
1000/1000 [==============================] - 138s 134ms/step - loss: 0.2161 - val_loss: 0.2159
Epoch 196/200
1000/1000 [==============================] - 149s 145ms/step - loss: 0.2161 - val_loss: 0.2159
Epoch 197/200
1000/1000 [==============================] - 133s 130ms/step - loss: 0.2161 - val_loss: 0.2158
Epoch 198/200
1000/1000 [==============================] - 145s 142ms/step - loss: 0.2161 - val_loss: 0.2159
Epoch 199/200
1000/1000 [==============================] - 135s 132ms/step - loss: 0.2161 - val_loss: 0.2161
Epoch 200/200
1000/1000 [==============================] - 139s 136ms/step - loss: 0.2160 - val_loss: 0.2162
2022-04-16 03:29:48.166443: W tensorflow/python/util/util.cc:368] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.
(sum_proj) autoencoder/16small